{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset using pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "\n",
      "First few duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [ResponseId, MainBranch, Age, Employment, RemoteWork, Check, CodingActivities, EdLevel, LearnCode, LearnCodeOnline, TechDoc, YearsCode, YearsCodePro, DevType, OrgSize, PurchaseInfluence, BuyNewTool, BuildvsBuy, TechEndorse, Country, Currency, CompTotal, LanguageHaveWorkedWith, LanguageWantToWorkWith, LanguageAdmired, DatabaseHaveWorkedWith, DatabaseWantToWorkWith, DatabaseAdmired, PlatformHaveWorkedWith, PlatformWantToWorkWith, PlatformAdmired, WebframeHaveWorkedWith, WebframeWantToWorkWith, WebframeAdmired, EmbeddedHaveWorkedWith, EmbeddedWantToWorkWith, EmbeddedAdmired, MiscTechHaveWorkedWith, MiscTechWantToWorkWith, MiscTechAdmired, ToolsTechHaveWorkedWith, ToolsTechWantToWorkWith, ToolsTechAdmired, NEWCollabToolsHaveWorkedWith, NEWCollabToolsWantToWorkWith, NEWCollabToolsAdmired, OpSysPersonal use, OpSysProfessional use, OfficeStackAsyncHaveWorkedWith, OfficeStackAsyncWantToWorkWith, OfficeStackAsyncAdmired, OfficeStackSyncHaveWorkedWith, OfficeStackSyncWantToWorkWith, OfficeStackSyncAdmired, AISearchDevHaveWorkedWith, AISearchDevWantToWorkWith, AISearchDevAdmired, NEWSOSites, SOVisitFreq, SOAccount, SOPartFreq, SOHow, SOComm, AISelect, AISent, AIBen, AIAcc, AIComplex, AIToolCurrently Using, AIToolInterested in Using, AIToolNot interested in Using, AINextMuch more integrated, AINextNo change, AINextMore integrated, AINextLess integrated, AINextMuch less integrated, AIThreat, AIEthics, AIChallenges, TBranch, ICorPM, WorkExp, Knowledge_1, Knowledge_2, Knowledge_3, Knowledge_4, Knowledge_5, Knowledge_6, Knowledge_7, Knowledge_8, Knowledge_9, Frequency_1, Frequency_2, Frequency_3, TimeSearching, TimeAnswering, Frustration, ProfessionalTech, ProfessionalCloud, ProfessionalQuestion, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 114 columns]\n",
      "\n",
      "Total rows involved in duplication: 0\n",
      "Sample of rows with duplicates (including first occurrences):\n",
      "Empty DataFrame\n",
      "Columns: [ResponseId, MainBranch, Age, Employment, RemoteWork, Check, CodingActivities, EdLevel, LearnCode, LearnCodeOnline, TechDoc, YearsCode, YearsCodePro, DevType, OrgSize, PurchaseInfluence, BuyNewTool, BuildvsBuy, TechEndorse, Country, Currency, CompTotal, LanguageHaveWorkedWith, LanguageWantToWorkWith, LanguageAdmired, DatabaseHaveWorkedWith, DatabaseWantToWorkWith, DatabaseAdmired, PlatformHaveWorkedWith, PlatformWantToWorkWith, PlatformAdmired, WebframeHaveWorkedWith, WebframeWantToWorkWith, WebframeAdmired, EmbeddedHaveWorkedWith, EmbeddedWantToWorkWith, EmbeddedAdmired, MiscTechHaveWorkedWith, MiscTechWantToWorkWith, MiscTechAdmired, ToolsTechHaveWorkedWith, ToolsTechWantToWorkWith, ToolsTechAdmired, NEWCollabToolsHaveWorkedWith, NEWCollabToolsWantToWorkWith, NEWCollabToolsAdmired, OpSysPersonal use, OpSysProfessional use, OfficeStackAsyncHaveWorkedWith, OfficeStackAsyncWantToWorkWith, OfficeStackAsyncAdmired, OfficeStackSyncHaveWorkedWith, OfficeStackSyncWantToWorkWith, OfficeStackSyncAdmired, AISearchDevHaveWorkedWith, AISearchDevWantToWorkWith, AISearchDevAdmired, NEWSOSites, SOVisitFreq, SOAccount, SOPartFreq, SOHow, SOComm, AISelect, AISent, AIBen, AIAcc, AIComplex, AIToolCurrently Using, AIToolInterested in Using, AIToolNot interested in Using, AINextMuch more integrated, AINextNo change, AINextMore integrated, AINextLess integrated, AINextMuch less integrated, AIThreat, AIEthics, AIChallenges, TBranch, ICorPM, WorkExp, Knowledge_1, Knowledge_2, Knowledge_3, Knowledge_4, Knowledge_5, Knowledge_6, Knowledge_7, Knowledge_8, Knowledge_9, Frequency_1, Frequency_2, Frequency_3, TimeSearching, TimeAnswering, Frustration, ProfessionalTech, ProfessionalCloud, ProfessionalQuestion, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count the number of duplicate rows in the dataset\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Display the first few duplicate rows\n",
    "duplicate_rows = df[df.duplicated(keep='first')]\n",
    "print(\"\\nFirst few duplicate rows:\")\n",
    "print(duplicate_rows.head())\n",
    "\n",
    "# Additional information: see which rows have duplicates (including first occurrences)\n",
    "all_duplicates = df[df.duplicated(keep=False)]\n",
    "print(f\"\\nTotal rows involved in duplication: {len(all_duplicates)}\")\n",
    "print(\"Sample of rows with duplicates (including first occurrences):\")\n",
    "print(all_duplicates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 65437 rows\n",
      "Dataset size after removing duplicates: 65437 rows\n",
      "Removed 0 duplicate rows\n",
      "Remaining duplicates after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Store the original size of the dataset\n",
    "original_size = len(df)\n",
    "print(f\"Original dataset size: {original_size} rows\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(f\"Dataset size after removing duplicates: {len(df_no_duplicates)} rows\")\n",
    "print(f\"Removed {original_size - len(df_no_duplicates)} duplicate rows\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "remaining_duplicates = df_no_duplicates.duplicated().sum()\n",
    "print(f\"Remaining duplicates after removal: {remaining_duplicates}\")\n",
    "\n",
    "# Alternative: Keep 'last' occurrence instead of the default 'first'\n",
    "# df_no_duplicates_last = df.drop_duplicates(keep='last')\n",
    "# print(f\"Dataset size keeping last occurrence: {len(df_no_duplicates_last)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values by column:\n",
      "ResponseId                 0\n",
      "MainBranch                 0\n",
      "Age                        0\n",
      "Employment                 0\n",
      "RemoteWork             10631\n",
      "                       ...  \n",
      "JobSatPoints_11        35992\n",
      "SurveyLength            9255\n",
      "SurveyEase              9199\n",
      "ConvertedCompYearly    42002\n",
      "JobSat                 36311\n",
      "Length: 114, dtype: int64\n",
      "\n",
      "Percentage of missing values by column:\n",
      "ResponseId              0.000000\n",
      "MainBranch              0.000000\n",
      "Age                     0.000000\n",
      "Employment              0.000000\n",
      "RemoteWork             16.246160\n",
      "                         ...    \n",
      "JobSatPoints_11        55.002522\n",
      "SurveyLength           14.143375\n",
      "SurveyEase             14.057796\n",
      "ConvertedCompYearly    64.186928\n",
      "JobSat                 55.490013\n",
      "Length: 114, dtype: float64\n",
      "\n",
      "Columns with more than 5% missing values:\n",
      "RemoteWork             16.246160\n",
      "CodingActivities       16.765744\n",
      "EdLevel                 7.110656\n",
      "LearnCode               7.563000\n",
      "LearnCodeOnline        24.756636\n",
      "                         ...    \n",
      "JobSatPoints_11        55.002522\n",
      "SurveyLength           14.143375\n",
      "SurveyEase             14.057796\n",
      "ConvertedCompYearly    64.186928\n",
      "JobSat                 55.490013\n",
      "Length: 109, dtype: float64\n",
      "\n",
      "Value counts for EdLevel:\n",
      "EdLevel\n",
      "Bachelor’s degree (B.A., B.S., B.Eng., etc.)                                          24942\n",
      "Master’s degree (M.A., M.S., M.Eng., MBA, etc.)                                       15557\n",
      "Some college/university study without earning a degree                                 7651\n",
      "Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)     5793\n",
      "Professional degree (JD, MD, Ph.D, Ed.D, etc.)                                         2970\n",
      "Associate degree (A.A., A.S., etc.)                                                    1793\n",
      "Primary/elementary school                                                              1146\n",
      "Something else                                                                          932\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most frequent education level: Bachelor’s degree (B.A., B.S., B.Eng., etc.)\n",
      "Missing values in EdLevel before imputation: 4653\n",
      "Missing values in EdLevel after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Identify missing values for all columns\n",
    "missing_values = df_no_duplicates.isnull().sum()\n",
    "print(\"Missing values by column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_percentage = (missing_values / len(df_no_duplicates)) * 100\n",
    "print(\"\\nPercentage of missing values by column:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "# Find columns with significant missing values\n",
    "significant_missing = missing_percentage[missing_percentage > 5]\n",
    "print(\"\\nColumns with more than 5% missing values:\")\n",
    "print(significant_missing)\n",
    "\n",
    "# Choose EdLevel column for imputation (or another column with significant missing values)\n",
    "# First, check the value counts to find the most frequent value\n",
    "print(\"\\nValue counts for EdLevel:\")\n",
    "ed_level_counts = df_no_duplicates['EdLevel'].value_counts()\n",
    "print(ed_level_counts)\n",
    "\n",
    "# Impute missing values with the most frequent value\n",
    "most_frequent_ed_level = df_no_duplicates['EdLevel'].mode()[0]\n",
    "print(f\"\\nMost frequent education level: {most_frequent_ed_level}\")\n",
    "\n",
    "# Count missing values in EdLevel before imputation\n",
    "missing_ed_level_before = df_no_duplicates['EdLevel'].isnull().sum()\n",
    "print(f\"Missing values in EdLevel before imputation: {missing_ed_level_before}\")\n",
    "\n",
    "# Impute missing values\n",
    "df_no_duplicates['EdLevel'] = df_no_duplicates['EdLevel'].fillna(most_frequent_ed_level)\n",
    "\n",
    "# Verify imputation\n",
    "missing_ed_level_after = df_no_duplicates['EdLevel'].isnull().sum()\n",
    "print(f\"Missing values in EdLevel after imputation: {missing_ed_level_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for ConvertedCompYearly:\n",
      "count    2.343500e+04\n",
      "mean     8.615529e+04\n",
      "std      1.867570e+05\n",
      "min      1.000000e+00\n",
      "25%      3.271200e+04\n",
      "50%      6.500000e+04\n",
      "75%      1.079715e+05\n",
      "max      1.625660e+07\n",
      "Name: ConvertedCompYearly, dtype: float64\n",
      "\n",
      "Missing values in ConvertedCompYearly: 42002\n",
      "Percentage of missing compensation data: 64.19%\n",
      "\n",
      "Rows after removing missing compensation: 23435\n",
      "Median yearly compensation: 65000.0\n",
      "\n",
      "Outlier threshold (upper bound): 220860.75\n",
      "Number of outliers in compensation data: 978\n",
      "Percentage of outliers: 1.49%\n",
      "\n",
      "Summary of normalized compensation data:\n",
      "count     23435.000000\n",
      "mean      77586.768786\n",
      "std       58421.337827\n",
      "min           1.000000\n",
      "25%       32712.000000\n",
      "50%       65000.000000\n",
      "75%      107971.500000\n",
      "max      220860.750000\n",
      "Name: ConvertedCompYearly, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Examine ConvertedCompYearly column\n",
    "print(\"Summary statistics for ConvertedCompYearly:\")\n",
    "print(df_no_duplicates['ConvertedCompYearly'].describe())\n",
    "\n",
    "# Check for missing values in ConvertedCompYearly\n",
    "missing_comp = df_no_duplicates['ConvertedCompYearly'].isnull().sum()\n",
    "print(f\"\\nMissing values in ConvertedCompYearly: {missing_comp}\")\n",
    "print(f\"Percentage of missing compensation data: {(missing_comp / len(df_no_duplicates)) * 100:.2f}%\")\n",
    "\n",
    "# Handle missing values in ConvertedCompYearly\n",
    "# Option 1: Remove rows with missing compensation (if analysis focuses on compensation)\n",
    "df_comp_complete = df_no_duplicates.dropna(subset=['ConvertedCompYearly'])\n",
    "print(f\"\\nRows after removing missing compensation: {len(df_comp_complete)}\")\n",
    "\n",
    "# Option 2: Impute with median (better than mean for salary data which often has outliers)\n",
    "median_comp = df_no_duplicates['ConvertedCompYearly'].median()\n",
    "print(f\"Median yearly compensation: {median_comp}\")\n",
    "df_no_duplicates['ConvertedCompYearly_imputed'] = df_no_duplicates['ConvertedCompYearly'].fillna(median_comp)\n",
    "\n",
    "# Check for outliers in compensation data\n",
    "q1 = df_no_duplicates['ConvertedCompYearly'].quantile(0.25)\n",
    "q3 = df_no_duplicates['ConvertedCompYearly'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "print(f\"\\nOutlier threshold (upper bound): {upper_bound}\")\n",
    "outliers = df_no_duplicates[df_no_duplicates['ConvertedCompYearly'] > upper_bound]\n",
    "print(f\"Number of outliers in compensation data: {len(outliers)}\")\n",
    "print(f\"Percentage of outliers: {(len(outliers) / len(df_no_duplicates)) * 100:.2f}%\")\n",
    "\n",
    "# Create normalized version for analysis without extreme outliers\n",
    "df_normalized = df_no_duplicates.copy()\n",
    "df_normalized.loc[df_normalized['ConvertedCompYearly'] > upper_bound, 'ConvertedCompYearly'] = upper_bound\n",
    "print(\"\\nSummary of normalized compensation data:\")\n",
    "print(df_normalized['ConvertedCompYearly'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned dataset information:\n",
      "Total rows: 65437\n",
      "Total columns: 115\n",
      "Total remaining missing values: 2886304\n",
      "\n",
      "Available columns for further analysis:\n",
      "- ResponseId\n",
      "- MainBranch\n",
      "- Age\n",
      "- Employment\n",
      "- RemoteWork\n",
      "- Check\n",
      "- CodingActivities\n",
      "- EdLevel\n",
      "- LearnCode\n",
      "- LearnCodeOnline\n",
      "- TechDoc\n",
      "- YearsCode\n",
      "- YearsCodePro\n",
      "- DevType\n",
      "- OrgSize\n",
      "- PurchaseInfluence\n",
      "- BuyNewTool\n",
      "- BuildvsBuy\n",
      "- TechEndorse\n",
      "- Country\n",
      "- Currency\n",
      "- CompTotal\n",
      "- LanguageHaveWorkedWith\n",
      "- LanguageWantToWorkWith\n",
      "- LanguageAdmired\n",
      "- DatabaseHaveWorkedWith\n",
      "- DatabaseWantToWorkWith\n",
      "- DatabaseAdmired\n",
      "- PlatformHaveWorkedWith\n",
      "- PlatformWantToWorkWith\n",
      "- PlatformAdmired\n",
      "- WebframeHaveWorkedWith\n",
      "- WebframeWantToWorkWith\n",
      "- WebframeAdmired\n",
      "- EmbeddedHaveWorkedWith\n",
      "- EmbeddedWantToWorkWith\n",
      "- EmbeddedAdmired\n",
      "- MiscTechHaveWorkedWith\n",
      "- MiscTechWantToWorkWith\n",
      "- MiscTechAdmired\n",
      "- ToolsTechHaveWorkedWith\n",
      "- ToolsTechWantToWorkWith\n",
      "- ToolsTechAdmired\n",
      "- NEWCollabToolsHaveWorkedWith\n",
      "- NEWCollabToolsWantToWorkWith\n",
      "- NEWCollabToolsAdmired\n",
      "- OpSysPersonal use\n",
      "- OpSysProfessional use\n",
      "- OfficeStackAsyncHaveWorkedWith\n",
      "- OfficeStackAsyncWantToWorkWith\n",
      "- OfficeStackAsyncAdmired\n",
      "- OfficeStackSyncHaveWorkedWith\n",
      "- OfficeStackSyncWantToWorkWith\n",
      "- OfficeStackSyncAdmired\n",
      "- AISearchDevHaveWorkedWith\n",
      "- AISearchDevWantToWorkWith\n",
      "- AISearchDevAdmired\n",
      "- NEWSOSites\n",
      "- SOVisitFreq\n",
      "- SOAccount\n",
      "- SOPartFreq\n",
      "- SOHow\n",
      "- SOComm\n",
      "- AISelect\n",
      "- AISent\n",
      "- AIBen\n",
      "- AIAcc\n",
      "- AIComplex\n",
      "- AIToolCurrently Using\n",
      "- AIToolInterested in Using\n",
      "- AIToolNot interested in Using\n",
      "- AINextMuch more integrated\n",
      "- AINextNo change\n",
      "- AINextMore integrated\n",
      "- AINextLess integrated\n",
      "- AINextMuch less integrated\n",
      "- AIThreat\n",
      "- AIEthics\n",
      "- AIChallenges\n",
      "- TBranch\n",
      "- ICorPM\n",
      "- WorkExp\n",
      "- Knowledge_1\n",
      "- Knowledge_2\n",
      "- Knowledge_3\n",
      "- Knowledge_4\n",
      "- Knowledge_5\n",
      "- Knowledge_6\n",
      "- Knowledge_7\n",
      "- Knowledge_8\n",
      "- Knowledge_9\n",
      "- Frequency_1\n",
      "- Frequency_2\n",
      "- Frequency_3\n",
      "- TimeSearching\n",
      "- TimeAnswering\n",
      "- Frustration\n",
      "- ProfessionalTech\n",
      "- ProfessionalCloud\n",
      "- ProfessionalQuestion\n",
      "- Industry\n",
      "- JobSatPoints_1\n",
      "- JobSatPoints_4\n",
      "- JobSatPoints_5\n",
      "- JobSatPoints_6\n",
      "- JobSatPoints_7\n",
      "- JobSatPoints_8\n",
      "- JobSatPoints_9\n",
      "- JobSatPoints_10\n",
      "- JobSatPoints_11\n",
      "- SurveyLength\n",
      "- SurveyEase\n",
      "- ConvertedCompYearly\n",
      "- JobSat\n",
      "- ConvertedCompYearly_imputed\n",
      "\n",
      "Compensation by years of professional coding experience:\n",
      "                       mean   median  count\n",
      "YearsCodePro                               \n",
      "1              33479.733539  23302.5    972\n",
      "10            106734.067489  77332.0   1541\n",
      "11            112980.133234  84069.0    668\n",
      "12            103740.947126  84000.0    870\n",
      "13            126421.296296  90000.0    567\n",
      "\n",
      "Data cleaning complete. Dataset is ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Final dataset summary\n",
    "print(\"Final cleaned dataset information:\")\n",
    "print(f\"Total rows: {len(df_no_duplicates)}\")\n",
    "print(f\"Total columns: {len(df_no_duplicates.columns)}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "remaining_missing = df_no_duplicates.isnull().sum().sum()\n",
    "print(f\"Total remaining missing values: {remaining_missing}\")\n",
    "\n",
    "# Display column names for reference\n",
    "print(\"\\nAvailable columns for further analysis:\")\n",
    "for col in df_no_duplicates.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Example of potential further analysis: Experience level vs. Compensation\n",
    "if 'YearsCodePro' in df_no_duplicates.columns and 'ConvertedCompYearly' in df_no_duplicates.columns:\n",
    "    experience_groups = df_no_duplicates.groupby('YearsCodePro')['ConvertedCompYearly'].agg(['mean', 'median', 'count'])\n",
    "    print(\"\\nCompensation by years of professional coding experience:\")\n",
    "    print(experience_groups.head())\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file (commented out by default)\n",
    "# df_no_duplicates.to_csv(\"cleaned_survey_data.csv\", index=False)\n",
    "print(\"\\nData cleaning complete. Dataset is ready for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "2116052544ce403759eef2159eb3d21f1d38e895d652bcaffa36a5791482361d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
